{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://editor.analyticsvidhya.com/uploads/94357telecom%20churn.png)","metadata":{}},{"cell_type":"markdown","source":"# **Introduction to the Telco Customer Churn Dataset**\n\nThe provided dataset contains information related to customers of a telecommunications company. It encompasses various attributes that offer insights into the characteristics and behaviors of these customers. The dataset is a valuable resource for conducting analyses and building predictive models to understand and anticipate customer churn.\n\n**Dataset Description**\n\nThe dataset comprises the following columns:\n\n- `customerID`: A unique identifier for each customer.\n- `gender`: The gender of the customer.\n- `SeniorCitizen`: Indicates if the customer is a senior citizen (1) or not (0).\n- `Partner`: Indicates if the customer has a partner (Yes) or not (No).\n- `Dependents`: Indicates if the customer has dependents (Yes) or not (No).\n- `tenure`: The duration, in months, that the customer has been with the company.\n- `PhoneService`: Indicates if the customer has phone service (Yes) or not (No).\n- `MultipleLines`: Indicates if the customer has multiple lines (Yes, No, or No phone service).\n- `InternetService`: The type of internet service the customer has (DSL, Fiber optic, or No).\n- `OnlineSecurity`: Indicates if the customer has online security (Yes, No, or No internet service).\n- `OnlineBackup`: Indicates if the customer has online backup (Yes, No, or No internet service).\n- `DeviceProtection`: Indicates if the customer has device protection (Yes, No, or No internet service).\n- `TechSupport`: Indicates if the customer has tech support (Yes, No, or No internet service).\n- `StreamingTV`: Indicates if the customer has streaming TV (Yes, No, or No internet service).\n- `StreamingMovies`: Indicates if the customer has streaming movies (Yes, No, or No internet service).\n- `Contract`: The contract term of the customer (Month-to-month, One year, Two year).\n- `PaperlessBilling`: Indicates if the customer has paperless billing (Yes) or not (No).\n- `PaymentMethod`: The method by which the customer makes payments.\n- `MonthlyCharges`: The monthly amount charged to the customer.\n- `TotalCharges`: The total amount charged to the customer.\n- `Churn`: Indicates if the customer churned (Yes) or not (No).\n\n**Objective**\n\nThe main objective of working with this dataset is to understand the factors that influence customer churn and to build predictive models that can help identify potential churners. By analyzing the provided attributes, we can uncover patterns and insights that may guide business decisions aimed at reducing customer churn and improving customer retention.\n\nThe dataset's attributes cover a range of customer-related aspects, from demographic information to service subscriptions and payment behavior. Analyzing this dataset can lead to valuable insights into customer behavior and preferences, which can ultimately contribute to more effective customer relationship management and strategic decision-making.\n\nIn the following sections, we will explore and preprocess the dataset, perform exploratory data analysis (EDA), and potentially build predictive models to achieve the stated objectives.","metadata":{}},{"cell_type":"markdown","source":"**Libraries Used for Machine Learning and Data Analysis**\n\nIn this code, several Python libraries are imported and utilized to perform machine learning tasks and data analysis. Here is a brief overview of the libraries used:\n\n**pandas**:\n`pandas` is a powerful data manipulation and analysis library. It provides data structures and functions for efficiently working with structured data, such as tables and CSV files.\n\n**numpy**:\n`numpy` is a fundamental package for scientific computing with Python. It provides support for large, multi-dimensional arrays and matrices, as well as a variety of mathematical functions.\n\n**matplotlib and seaborn**:\n`matplotlib` and `seaborn` are plotting libraries that allow you to create various types of visualizations, such as histograms, scatter plots, line plots, and more, to help you better understand your data.\n\n**warnings**:\nThe `warnings` module is used to control and suppress warning messages that might be generated during the code execution. In this code, it is used to filter out specific warning messages.\n\n**sklearn**:\n`scikit-learn`, often referred to as `sklearn`, is a widely used machine learning library. It provides tools for classification, regression, clustering, dimensionality reduction, and more.\n\n**imblearn**:\n`imblearn` is a library for handling imbalanced datasets in machine learning. It includes techniques for oversampling, undersampling, and generating synthetic samples to address class imbalance.\n\n**LabelEncoder, OneHotEncoder, RobustScaler, StandardScaler, MinMaxScaler**:\nThese are preprocessing tools from `sklearn` that are used to transform and scale data before feeding it into machine learning algorithms. `LabelEncoder` is used for encoding categorical variables, while scalers like `RobustScaler`, `StandardScaler`, and `MinMaxScaler` normalize and scale numeric features.\n\n**CatBoost, LightGBM, RandomForest, GradientBoosting, LogisticRegression, KNeighbors, SVC, DecisionTree, XGBoost**:\nThese are machine learning algorithms provided by `catboost`, `lightgbm`, `scikit-learn`, and `xgboost` libraries. They are used for building and training predictive models for classification tasks.\n\n**confusion_matrix**:\n`confusion_matrix` is a function from `sklearn.metrics` that calculates the confusion matrix for evaluating the performance of classification models.\n\n**train_test_split, cross_validate, GridSearchCV**:\nThese functions from `sklearn.model_selection` are used for data splitting, cross-validation, and hyperparameter tuning, respectively. They help in optimizing and assessing the performance of machine learning models.\n\n**logging**:\n`logging` is a built-in Python module used for generating log messages during code execution. It can be used to capture important information, warnings, and errors.\n\nOverall, these libraries collectively provide a comprehensive set of tools and functions for data preprocessing, modeling, evaluation, and visualization in machine learning and data analysis tasks.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=UserWarning)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:44:43.025045Z","iopub.execute_input":"2023-08-11T16:44:43.027559Z","iopub.status.idle":"2023-08-11T16:44:43.717210Z","shell.execute_reply.started":"2023-08-11T16:44:43.027521Z","shell.execute_reply":"2023-08-11T16:44:43.716157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, RobustScaler, StandardScaler, MinMaxScaler\nfrom catboost import CatBoostClassifier\nimport logging\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split, cross_validate, GridSearchCV","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:44:43.723282Z","iopub.execute_input":"2023-08-11T16:44:43.723937Z","iopub.status.idle":"2023-08-11T16:44:45.008144Z","shell.execute_reply.started":"2023-08-11T16:44:43.723901Z","shell.execute_reply":"2023-08-11T16:44:45.007131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Configuration Class (CFG) for Data Preprocessing**\n\nThe `CFG` class is a configuration class designed to control various data preprocessing steps in a machine learning pipeline. By modifying the attributes within this class, you can easily customize the data preprocessing techniques applied to your dataset before training a machine learning model. Let's explore the attributes and their meanings in more detail.\n\n**Outlier Handling**\n- `outlier_clipper`: If set to `True`, outliers in the data will be clipped to a specified range.\n- `outlier_remover`: If set to `True`, outliers will be removed from the dataset.\n- `outlier_replacer`: If set to `True`, outliers will be replaced with a central value (e.g., mean or median).\n\n**Encoding Techniques**\n- `one_hot_encoder`: If set to `True`, categorical variables will be one-hot encoded.\n- `label_encoder`: If set to `True`, categorical variables will be label encoded.\n- `ordinal_encoder`: If set to `True`, ordinal categorical variables will be encoded.\n\n**Feature Scaling**\n- `min_max_scaler`: If set to `True`, data will be scaled using Min-Max scaling.\n- `robust_scaler`: If set to `True`, data will be scaled using Robust scaling.\n- `standard_scaler`: If set to `True`, data will be scaled using Standard scaling.\n\n**How to Use the CFG Class**\n\nTo utilize the `CFG` class for data preprocessing, follow these steps:\n\n1. Modify the attributes within the `CFG` class according to your preprocessing requirements. For example:\n\n   CFG.outlier_clipper = True\n   \n   CFG.label_encoder = True\n   \n   CFG.robust_scaler = True\n  \n\n2. Use the `CFG` attributes in your data preprocessing pipeline. For instance:\n\n    if CFG.outlier_clipper:\n    \n    ***Apply outlier clipping codes***\n\n    if CFG.label_encoder:\n    \n    ***Apply label encoding codes***\n \n    if CFG.robust_scaler:\n    \n    ***Apply Robust scaling codes***\n\n","metadata":{}},{"cell_type":"code","source":"class CFG:\n    outlier_clipper = False\n    outlier_remover = False\n    outlier_replacer = False\n    \n    one_hot_encoder = False\n    label_encoder = True\n    ordinal_encoder = True\n    \n    min_max_scaler = False\n    robust_scaler = True\n    standard_scaler = False","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:44:45.009463Z","iopub.execute_input":"2023-08-11T16:44:45.010269Z","iopub.status.idle":"2023-08-11T16:44:45.017474Z","shell.execute_reply.started":"2023-08-11T16:44:45.010233Z","shell.execute_reply":"2023-08-11T16:44:45.015201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option(\"display.max_columns\",500)\nchurn_data = pd.read_csv(\"/kaggle/input/churn-dataset/Telco-Customer-Churn.csv\")\ndf = churn_data.copy()\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:44:45.021206Z","iopub.execute_input":"2023-08-11T16:44:45.021511Z","iopub.status.idle":"2023-08-11T16:44:45.087742Z","shell.execute_reply.started":"2023-08-11T16:44:45.021486Z","shell.execute_reply":"2023-08-11T16:44:45.086608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(\"customerID\", axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:44:45.089535Z","iopub.execute_input":"2023-08-11T16:44:45.089907Z","iopub.status.idle":"2023-08-11T16:44:45.098023Z","shell.execute_reply.started":"2023-08-11T16:44:45.089869Z","shell.execute_reply":"2023-08-11T16:44:45.096928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_df(data, head=5):\n    print(\"\\n******Shape******\")\n    print(f'Shape     : {df.shape}\\n')\n    print(\"\\n******Info******\")\n    print(data.info())\n    print(\"\\n******Types******\")\n    print(data.dtypes)\n    print(\"\\n******Head******\")\n    print(data.head(head))\n    print(\"\\n******Missing Values******\")\n    print(data.isnull().sum())\n    print(\"\\n******Duplicated Values******\")\n    print(data.duplicated().sum())\n    print(\"\\n******Unique Values******\")\n    print(data.nunique())\n    print(\"\\n******Describe******\")\n    print(data.describe(percentiles=[0.05, 0.25, 0.5, 0.75, 0.95]).T)\n    \ncheck_df(df)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:44:45.099812Z","iopub.execute_input":"2023-08-11T16:44:45.100225Z","iopub.status.idle":"2023-08-11T16:44:45.268316Z","shell.execute_reply.started":"2023-08-11T16:44:45.100162Z","shell.execute_reply":"2023-08-11T16:44:45.267243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in df.columns:\n    if df[col].dtype in ['object','category']:\n        filtered_values = [value for value in df[col] if pd.notnull(value) and value < '0']\n        filtered_values.sort()\n        print(col, \"=\" ,filtered_values)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:44:45.270113Z","iopub.execute_input":"2023-08-11T16:44:45.270522Z","iopub.status.idle":"2023-08-11T16:44:45.429748Z","shell.execute_reply.started":"2023-08-11T16:44:45.270484Z","shell.execute_reply":"2023-08-11T16:44:45.428795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"TotalCharges\"] = df[\"TotalCharges\"].replace(' ', np.nan)\ndf[\"TotalCharges\"] = df[\"TotalCharges\"].astype(float)\ndf.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:44:45.431179Z","iopub.execute_input":"2023-08-11T16:44:45.431544Z","iopub.status.idle":"2023-08-11T16:44:45.488085Z","shell.execute_reply.started":"2023-08-11T16:44:45.431516Z","shell.execute_reply":"2023-08-11T16:44:45.484243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Visualizing Null and Non-Null Values**\n\nIn this section, a visualization is created to illustrate the distribution of null and non-null values for each column in the DataFrame (`df`). The code provided generates a horizontal bar chart using `matplotlib` to display the percentage of null and non-null values for each column.\n\n**1. Creating Null Value DataFrame**\n\nThe code begins by calculating the count of null values for each column using the `.isnull().sum()` method on the DataFrame. The result is transformed into a DataFrame named `df_null_values`, with columns 'Count', 'Porcentaje_nulos', and 'Porcentaje_no_nulos'. The percentage of null and non-null values is calculated based on the total number of rows.\n\n**2. Generating the Bar Chart**\n\nThe horizontal bar chart is created using the `matplotlib` library. The `plt.subplots()` function initializes a figure and axes for plotting. The heights of the bars represent the percentages of null and non-null values for each column.\n\nTwo sets of horizontal bars are plotted:\n- `rects1`: Represents the percentage of null values (colored in red).\n- `rects2`: Represents the percentage of non-null values (colored in orange).\n\nAxis labels, ticks, title, and legend are added to enhance the visualization. The `autolabel()` function is defined to add percentage labels to the bars.\n\nFinally, the visualization is displayed using `plt.tight_layout()` and `plt.show()`.\n","metadata":{}},{"cell_type":"code","source":"df_null_values = df.isnull().sum().to_frame().rename(columns={0: 'Count'})\ndf_null_values['Percentage_nulls'] = (df_null_values['Count'] / len(df)) * 100\ndf_null_values['Percentage_no_nulls'] = 100 - df_null_values['Percentage_nulls']\n\nn = len(df_null_values.index)\nx = np.arange(n)\n\nfig, ax = plt.subplots(figsize=(12, 8))\n\nbar_width = 0.4\ngap = 0.2\n\nrects1 = ax.barh(x - gap / 2, df_null_values['Percentage_nulls'], bar_width, label='Null values', color='red')\nrects2 = ax.barh(x + gap / 2, df_null_values['Percentage_no_nulls'], bar_width, label='No null values', color='orange')\n\nax.set_title('Null Values and Non-null Values', fontsize=15, fontweight='bold')\nax.set_xlabel('% Percentage', fontsize=12, fontweight='bold')\nax.set_yticks(x)\nax.set_yticklabels(df_null_values.index, fontsize=10, fontweight='bold') \n\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.legend()\n\ndef autolabel(rects):\n    for rect in rects:\n        width = rect.get_width()\n        ax.annotate(f'{width:.2f}%',\n                    xy=(width, rect.get_y() + rect.get_height() / 2),\n                    xytext=(2, 0),\n                    textcoords=\"offset points\",\n                    ha='left', va='center', size=10, weight='bold')\n\nautolabel(rects1)\nautolabel(rects2)\n\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:44:45.489637Z","iopub.execute_input":"2023-08-11T16:44:45.492984Z","iopub.status.idle":"2023-08-11T16:44:46.413115Z","shell.execute_reply.started":"2023-08-11T16:44:45.492944Z","shell.execute_reply":"2023-08-11T16:44:46.412180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in df.columns:\n    print(df[col].value_counts(),\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:44:46.414105Z","iopub.execute_input":"2023-08-11T16:44:46.414478Z","iopub.status.idle":"2023-08-11T16:44:46.457461Z","shell.execute_reply.started":"2023-08-11T16:44:46.414442Z","shell.execute_reply":"2023-08-11T16:44:46.456477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.nunique()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:44:46.458919Z","iopub.execute_input":"2023-08-11T16:44:46.459278Z","iopub.status.idle":"2023-08-11T16:44:46.481509Z","shell.execute_reply.started":"2023-08-11T16:44:46.459245Z","shell.execute_reply":"2023-08-11T16:44:46.480359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def grab_col_names(dataframe, cat_th=10, car_th=20, print_results=True):\n\n    '''\n   Returns the names of categorical, numeric and categorical but cardinal variables in the data set.\n   Note Categorical variables include categorical variables with numeric appearance.\n\n    Parameters\n    ------\n        dataframe: dataframe\n                Dataframe to get variable names\n        cat_th: int, optional\n                class threshold for numeric but categorical variables\n        car_th: int, optinal\n                class threshold for categorical but cardinal variables\n    Returns\n    ------\n        cat_cols: list\n                Categorical variable list\n        num_cols: list\n                Numeric variable list\n        cat_but_car: list\n                List of cardinal variables with categorical view\n    Examples\n    ------\n        import seaborn as sns\n        df = sns.load_dataset(\"iris\")\n        print(grab_col_names(df))\n\n    Notes\n    ------\n        cat_cols + num_cols + cat_but_car = total number of variables\n        The sum of 3 lists with return equals the total number of variables: cat_cols + num_cols + cat_but_car = number of variables\n    '''\n\n    cat_cols = [col for col in dataframe.columns if str(dataframe[col].dtypes) in [\"category\", \"object\", \"bool\"]]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < 10 and dataframe[col].dtypes in [\"int64\", \"float64\"]]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > 20 and str(dataframe[col].dtypes) in [\"category\", \"object\"]]\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n    cat_cols = cat_cols + num_but_cat\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes in [\"int64\", \"float64\"]]\n    num_cols = [col for col in num_cols if col not in cat_cols]\n\n    if print_results:\n        print(f'Observations {dataframe.shape[0]}')\n        print(f'Variables:  {dataframe.shape[1]}')\n        print(f'cat_cols:  {len(cat_cols)}')\n        print(f'num_cols:  {len(num_cols)}')\n        print(f'cat_but_car:  {len(cat_but_car)}')\n        print(f'num_but_cat:  {len(num_but_cat)}')\n\n    return cat_cols, num_cols, cat_but_car\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:44:46.483309Z","iopub.execute_input":"2023-08-11T16:44:46.483709Z","iopub.status.idle":"2023-08-11T16:44:46.520643Z","shell.execute_reply.started":"2023-08-11T16:44:46.483674Z","shell.execute_reply":"2023-08-11T16:44:46.519471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Function to Calculate Outlier Thresholds**\n\nThe provided Python function, `outlier_thresholds`, is designed to calculate the lower and upper outlier thresholds for a given column in a DataFrame. These thresholds are essential for identifying potential outliers within the data. The function employs the Interquartile Range (IQR) method, a robust statistical measure, to determine the threshold values.\n","metadata":{}},{"cell_type":"code","source":"def outlier_thresholds(dataframe, col, low_quantile=0.25, up_quantile=0.75):\n    \n    quantile_one = dataframe[col].quantile(low_quantile)\n    quantile_three = dataframe[col].quantile(up_quantile)\n    \n    interquantile_range = quantile_three - quantile_one\n    up_limit = quantile_three + 1.5 * interquantile_range\n    low_limit = quantile_one - 1.5 * interquantile_range\n    \n    return  low_limit, up_limit","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:44:46.526563Z","iopub.execute_input":"2023-08-11T16:44:46.526866Z","iopub.status.idle":"2023-08-11T16:44:46.533379Z","shell.execute_reply.started":"2023-08-11T16:44:46.526838Z","shell.execute_reply":"2023-08-11T16:44:46.532050Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Outlier Detection Function and Usage Example**\n\nThe provided Python function, `check_outlier`, is designed to detect outliers in a specific column of a DataFrame based on the calculated outlier thresholds. The function utilizes the `outlier_thresholds` function previously described to determine if there are any data points outside the calculated threshold range.","metadata":{}},{"cell_type":"code","source":"def check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False\n    \nfor col in num_cols:\n    print(f\"{col}, outlier detection is {check_outlier(df, col)}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:44:46.535506Z","iopub.execute_input":"2023-08-11T16:44:46.536333Z","iopub.status.idle":"2023-08-11T16:44:46.562500Z","shell.execute_reply.started":"2023-08-11T16:44:46.536294Z","shell.execute_reply":"2023-08-11T16:44:46.561436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Box Plot Visualization Function and Usage Example**\n\nThe provided Python function, `plot_boxplots`, is designed to create box plot visualizations for a set of numeric columns in a DataFrame. Box plots are useful for visualizing the distribution, central tendency, and spread of data within each column.\n\n**How It Works**\n1. The function calculates the number of rows (`nrows`) required to accommodate all the specified numeric columns in the plot grid.\n2. It creates a subplot grid with the specified number of rows and `ncols`.\n3. For each numeric column, a box plot is generated using `sns.boxplot`, and the resulting plot is placed in the corresponding subplot.\n4. The title of each subplot is set to indicate the column name.\n","metadata":{}},{"cell_type":"code","source":"def plot_boxplots(dataframe, num_cols, ncols=2):\n    nrows = (len(num_cols) + ncols - 1) // ncols\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 5*nrows))\n    \n    colors = sns.color_palette('Set3', n_colors=len(dataframe.columns))\n    \n    for i, col in enumerate(num_cols):\n        if col in dataframe.columns:\n            ax = axes[i // ncols, i % ncols]\n            sns.boxplot(x=dataframe[col], ax=ax, color=colors[i])\n            ax.set_title(f\"Box Plot of {col}\")\n\n    plt.tight_layout()\n    plt.show()\n\nplot_boxplots(df, num_cols)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:44:46.564130Z","iopub.execute_input":"2023-08-11T16:44:46.564518Z","iopub.status.idle":"2023-08-11T16:44:47.301265Z","shell.execute_reply.started":"2023-08-11T16:44:46.564484Z","shell.execute_reply":"2023-08-11T16:44:47.300312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def correlation_matrix(dataframe):\n    num_cols = dataframe.select_dtypes(include=[int,float]).columns\n\n    variables = num_cols\n    data = dataframe[variables]\n    correlation_matrix = data.corr()\n    plt.figure(figsize=(12, 8))\n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".3f\", linewidths=1)\n    plt.title('Correlation Matrix')\n    \ncorrelation_matrix(df)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:44:47.302919Z","iopub.execute_input":"2023-08-11T16:44:47.303300Z","iopub.status.idle":"2023-08-11T16:44:47.718307Z","shell.execute_reply.started":"2023-08-11T16:44:47.303265Z","shell.execute_reply":"2023-08-11T16:44:47.717351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_target(dataframe, column):\n    my_pal = {'No': 'deepskyblue', 'Yes': 'deeppink'}\n\n    plt.figure(figsize = (12, 6))\n    ax = sns.countplot(x = 'Churn', data = df, palette = my_pal)\n    plt.title('Class Distribution')\n    plt.show()\n\n    Count_Normal_transacation = len(df[df['Churn']=='No'])\n    Count_Fraud_transacation = len(df[df['Churn']=='Yes'])\n    Percentage_of_Normal_transacation = Count_Normal_transacation/(Count_Normal_transacation+Count_Fraud_transacation)\n    print('% of no values       :', Percentage_of_Normal_transacation*100)\n    print('Number of no total values     :', Count_Normal_transacation)\n    Percentage_of_Fraud_transacation= Count_Fraud_transacation/(Count_Normal_transacation+Count_Fraud_transacation)\n    print('% of yes values         :',Percentage_of_Fraud_transacation*100)\n    print('Number of yes total values    :', Count_Fraud_transacation)\n\nplot_target(df,'Churn')","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:44:47.719969Z","iopub.execute_input":"2023-08-11T16:44:47.720721Z","iopub.status.idle":"2023-08-11T16:44:47.979947Z","shell.execute_reply.started":"2023-08-11T16:44:47.720682Z","shell.execute_reply":"2023-08-11T16:44:47.979024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Segmentation of Data Based on 'tenure' Column**\n\nThe provided code snippet demonstrates how to create a new column called 'segment_class' in a DataFrame by segmenting the data based on the 'tenure' column using quantiles. This segmentation assigns each data point to one of the specified segments or labels.\n\n**Code Explanation**\n\n1. `num_segments = 3`: The number of segments or quantiles you want to create.\n\n2. `labels = [3, 2, 1]`: The labels assigned to each segment. These labels will be assigned based on the quantile ranges, where the higher label corresponds to higher 'tenure' values.\n\n3. `pd.qcut(df['tenure'], q=num_segments, labels=labels)`: The `qcut` function is used to perform quantile-based discretization of the 'tenure' column. It divides the data into the specified number of quantiles (segments) and assigns labels based on the provided labels list.","metadata":{}},{"cell_type":"code","source":"num_segments = 3\nlabels = [3,2,1]\ndf['segment_class'] = pd.qcut(df['tenure'], q=num_segments, labels=labels)\ndf['segment_class'] = df['segment_class'].astype(float)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:44:47.982192Z","iopub.execute_input":"2023-08-11T16:44:47.983181Z","iopub.status.idle":"2023-08-11T16:44:48.011972Z","shell.execute_reply.started":"2023-08-11T16:44:47.983143Z","shell.execute_reply":"2023-08-11T16:44:48.011063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Function to Calculate Total Services and Apply to DataFrame**\n\nThe provided code defines a Python function `total_service` that calculates the total number of services subscribed by a customer based on various criteria related to services such as phone service, online security, and online backup. The function is then applied to each row of a DataFrame to create a new column called \"Total_Services.\"\n\n**Function Explanation**\n\nThe `total_service` function takes a row of data as input and applies specific rules to determine the total number of services subscribed by a customer.\n\n**Rules and Criteria for Service Calculation**\n\nThe function uses a series of conditions and logical operators to determine the total number of services:\n\n- If the customer has phone service, online security, online backup, and either DSL or fiber optic internet service, the total is set to 4.\n- If the customer has phone service, online security, and online backup, but no internet service, or has phone service, online security, online backup, and either DSL or fiber optic internet service, the total is set to 3.\n- If the customer has phone service and either online security or online backup, with or without internet service, or has phone service, online security, online backup, and either DSL or fiber optic internet service, the total is set to 2.\n- If the customer has phone service and either online security or online backup, with or without internet service, or has phone service, online security, online backup, and either DSL or fiber optic internet service, the total is set to 1.\n- If none of the above conditions are met, the total is set to 0.","metadata":{}},{"cell_type":"code","source":"def total_service(row):\n    \n    if (row[\"PhoneService\"] == \"Yes\" and row[\"OnlineSecurity\"] == \"Yes\" and row[\"OnlineBackup\"] == \"Yes\" and\n       (row[\"InternetService\"]==\"DSL\" or row[\"InternetService\"]==\"Fiber optic\")):\n        return 4\n    \n    elif ((row[\"PhoneService\"] == \"Yes\" and row[\"OnlineSecurity\"] == \"Yes\" and row[\"OnlineBackup\"] == \"Yes\" and\n          row[\"InternetService\"]==\"No\") or\n          \n          (row[\"PhoneService\"] == \"Yes\" and row[\"OnlineSecurity\"] == \"Yes\" and row[\"OnlineBackup\"] == \"No\" and\n          (row[\"InternetService\"]==\"DSL\" or row[\"InternetService\"]==\"Fiber optic\")) or\n          \n          (row[\"PhoneService\"] == \"Yes\" and row[\"OnlineSecurity\"] == \"No\" and row[\"OnlineBackup\"] == \"Yes\" and\n          (row[\"InternetService\"]==\"DSL\" or row[\"InternetService\"]==\"Fiber optic\")) or \n          \n          (row[\"PhoneService\"] == \"No\" and row[\"OnlineSecurity\"] == \"Yes\" and row[\"OnlineBackup\"] == \"Yes\" and\n          (row[\"InternetService\"]==\"DSL\" or row[\"InternetService\"]==\"Fiber optic\"))):\n        return 3\n    \n    elif ((row[\"PhoneService\"] == \"Yes\" and row[\"OnlineSecurity\"] == \"Yes\" and row[\"OnlineBackup\"] == \"No\" and\n          row[\"InternetService\"]==\"No\") or\n          \n          (row[\"PhoneService\"] == \"Yes\" and row[\"OnlineSecurity\"] == \"No\" and row[\"OnlineBackup\"] == \"Yes\" and\n           row[\"InternetService\"]==\"No\") or\n          \n          (row[\"PhoneService\"] == \"No\" and row[\"OnlineSecurity\"] == \"Yes\" and row[\"OnlineBackup\"] == \"Yes\" and\n           row[\"InternetService\"]==\"No\") or \n          \n          (row[\"PhoneService\"] == \"No\" and row[\"OnlineSecurity\"] == \"No\" and row[\"OnlineBackup\"] == \"Yes\" and\n          (row[\"InternetService\"]==\"DSL\" or row[\"InternetService\"]==\"Fiber optic\")) or\n          \n          (row[\"PhoneService\"] == \"No\" and row[\"OnlineSecurity\"] == \"Yes\" and row[\"OnlineBackup\"] == \"No\" and\n          (row[\"InternetService\"]==\"DSL\" or row[\"InternetService\"]==\"Fiber optic\")) or\n         \n          (row[\"PhoneService\"] == \"Yes\" and row[\"OnlineSecurity\"] == \"No\" and row[\"OnlineBackup\"] == \"No\" and\n          (row[\"InternetService\"]==\"DSL\" or row[\"InternetService\"]==\"Fiber optic\"))):\n        return 2\n    \n    elif ((row[\"PhoneService\"] == \"Yes\" and row[\"OnlineSecurity\"] == \"No\" and row[\"OnlineBackup\"] == \"No\" and\n          row[\"InternetService\"]==\"No\") or\n          \n          (row[\"PhoneService\"] == \"No\" and row[\"OnlineSecurity\"] == \"Yes\" and row[\"OnlineBackup\"] == \"No\" and\n          row[\"InternetService\"]==\"No\") or\n          \n          (row[\"PhoneService\"] == \"No\" and row[\"OnlineSecurity\"] == \"No\" and row[\"OnlineBackup\"] == \"Yes\" and\n          row[\"InternetService\"]==\"No\") or \n          \n          (row[\"PhoneService\"] == \"No\" and row[\"OnlineSecurity\"] == \"No\" and row[\"OnlineBackup\"] == \"No\" and\n          (row[\"InternetService\"]==\"DSL\" or row[\"InternetService\"]==\"Fiber optic\"))):\n        return 1\n    else:\n        return 0\n\n    \ndf[\"Total_Services\"] = df.apply(total_service, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:44:48.013480Z","iopub.execute_input":"2023-08-11T16:44:48.014044Z","iopub.status.idle":"2023-08-11T16:44:48.625763Z","shell.execute_reply.started":"2023-08-11T16:44:48.013982Z","shell.execute_reply":"2023-08-11T16:44:48.624760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Function to Calculate Total Streaming Services and Apply to DataFrame**\n\nThe provided code defines a Python function `total_streaming` that calculates the total number of streaming services subscribed by a customer based on their usage of streaming TV and streaming movies. The function is then applied to each row of a DataFrame to create a new column called \"Total_Streaming.\"\n\n**Function Explanation**\n\nThe `total_streaming` function takes a row of data as input and applies specific rules to determine the total number of streaming services subscribed by a customer.\n\n**Rules and Criteria for Streaming Service Calculation**\n\nThe function uses conditions and logical operators to determine the total number of streaming services:\n\n- If the customer has both streaming TV and streaming movies, the total is set to 2.\n- If the customer has either streaming TV or streaming movies, the total is set to 1.\n- If none of the above conditions are met, the total is set to 0.","metadata":{}},{"cell_type":"code","source":"def total_streaming(row):\n    if row[\"StreamingTV\"]==\"Yes\" and row[\"StreamingMovies\"]==\"Yes\":\n        return 2\n    elif ((row[\"StreamingTV\"]==\"Yes\" and row[\"StreamingMovies\"]==\"No\") or \n         (row[\"StreamingTV\"]==\"No\" and row[\"StreamingMovies\"]==\"Yes\")):\n        return 1\n    else:\n        return 0\n    \ndf[\"Total_Streaming\"] = df.apply(total_streaming, axis=1)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:44:48.627073Z","iopub.execute_input":"2023-08-11T16:44:48.627544Z","iopub.status.idle":"2023-08-11T16:44:48.807498Z","shell.execute_reply.started":"2023-08-11T16:44:48.627508Z","shell.execute_reply":"2023-08-11T16:44:48.806323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_features(dataframe):\n    feature_columns = ['segment_class','Total_Services','Total_Streaming']\n    num_plots = len(feature_columns)\n    num_cols = 2\n    num_rows = (num_plots - 1) // num_cols + 1\n    \n    plt.figure(figsize=(15, 12))\n    \n    for i, feature in enumerate(feature_columns, 1):\n        plt.subplot(num_rows, num_cols, i)\n        ax = sns.countplot(x=feature, data=dataframe, palette='coolwarm')\n        plt.xlabel('')\n        plt.ylabel('Count')\n        plt.title(f'Distribution of {feature}')\n        \n        for p in ax.patches:\n                height = p.get_height()\n                ax.annotate(f'{height}', (p.get_x() + p.get_width() / 2., height), \n                            ha='center', va='bottom', fontsize=10, color='black')\n    \n    plt.tight_layout()\n    plt.show()\n\nvisualize_features(df)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:44:48.813178Z","iopub.execute_input":"2023-08-11T16:44:48.813592Z","iopub.status.idle":"2023-08-11T16:44:49.581362Z","shell.execute_reply.started":"2023-08-11T16:44:48.813565Z","shell.execute_reply":"2023-08-11T16:44:49.580458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Preprocessing: Outlier Handling**\n\nIn this section, we define a `DataProcessorOutlier` class responsible for preprocessing the data to handle outliers based on a given configuration (`cfg`). This class encapsulates methods to clip, remove, or replace outliers in the input DataFrame.\n\n**1. Outlier Clipping (`outlier_clipper`)**\n\nThe `outlier_clipper` method clips the extreme values of numerical columns within the interquartile range (IQR). For each numerical column, it calculates the first and third quartiles (`q1_val` and `q3_val`) and then clips the column's values to be within this range.\n\n**2. Outlier Removal (`outlier_remover`)**\n\nThe `outlier_remover` method removes outliers by filtering out rows where numerical column values fall outside a predefined range (`[q1_val, q3_val]`) for each column.\n\n**3. Outlier Replacement (`outlier_replacer`)**\n\nThe `outlier_replacer` method replaces outliers with the median value for each numerical column. It utilizes the same IQR-based range as in the previous methods.\n\nAfter defining these outlier handling methods, the `process_data` function takes a DataFrame as input and applies the specified outlier handling techniques based on the provided configuration (`cfg`).\n\nFinally, an instance of the `DataProcessorOutlier` class is created with a configuration object (`CFG()`), and the `process_data` method is called on the input DataFrame `df`.","metadata":{}},{"cell_type":"code","source":"class DataProcessorOutlier:\n    def __init__(self, cfg):\n        self.cfg = cfg\n\n    def process_data(self, dataframe):\n        if self.cfg.outlier_clipper:\n            for col in dataframe.columns:\n                if dataframe[col].dtypes not in ['object', 'category']:\n                    q1_val = dataframe[col].quantile(0.25)\n                    q3_val = dataframe[col].quantile(0.75)\n                    dataframe.loc[:, col] = dataframe[col].clip(lower=q1_val, upper=q3_val)\n\n        if self.cfg.outlier_remover:\n            for col in dataframe.columns:\n                if dataframe[col].dtypes not in ['object', 'category']:\n                    q1_val = dataframe[col].quantile(0.07)\n                    q3_val = dataframe[col].quantile(0.93)\n                    dataframe = dataframe.loc[(dataframe[col] >= q1_val) & (processed_data[col] <= q3_val)]\n                    \n        if self.cfg.outlier_replacer:\n            for col in dataframe.columns:\n                if dataframe[col].dtypes not in ['object', 'category']:\n                    median_val = dataframe[col].median()\n                    dataframe[col] = dataframe[col].where((dataframe[col] >= q1_val) & (dataframe[col] <= q3_val), median_val)\n\n\n        return dataframe.head()\n\n\ndata_processor = DataProcessorOutlier(CFG())\ndata_processor.process_data(df)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:44:49.582825Z","iopub.execute_input":"2023-08-11T16:44:49.583920Z","iopub.status.idle":"2023-08-11T16:44:49.620886Z","shell.execute_reply.started":"2023-08-11T16:44:49.583880Z","shell.execute_reply":"2023-08-11T16:44:49.619890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Preprocessing: Encoding Categorical Variables**\n\nIn this section, we define a `DataProcessorEncode` class responsible for encoding categorical variables within a given DataFrame. The class offers different encoding techniques based on the provided configuration (`cfg`).\n\n**1. One-Hot Encoding (`one_hot_encoder`)**\n\nThe `one_hot_encoder` method applies one-hot encoding to categorical variables with more than two unique values. It identifies such columns (`object_cols`) and creates binary columns for each unique category using the `pd.get_dummies` function. If applicable columns are found, one-hot encoding is applied; otherwise, a warning is issued.\n\n**2. Label Encoding (`label_encoder`)**\n\nThe `label_encoder` method applies label encoding to categorical variables with exactly two unique values. It uses the `LabelEncoder` from scikit-learn to transform binary categorical columns into numerical values (0 and 1). Similar to one-hot encoding, a warning is issued if no applicable columns are found.\n\n**3. Ordinal Encoding (`ordinal_encoder`)**\n\nThe `ordinal_encoder` method performs ordinal encoding for all categorical columns. It maps unique category values to incremental integer values, effectively transforming categorical variables into ordinal representations. A dictionary (`ordinal_encoder`) is created to store the mapping for each categorical column.\n\nAfter defining these encoding methods, the `encode_data` function takes a DataFrame as input and applies the specified encoding techniques based on the provided configuration (`cfg`).\n\nFinally, an instance of the `DataProcessorEncode` class is created with a configuration object (`CFG()`), and the `encode_data` method is called on the input DataFrame `df`. The encoded DataFrame `df` is returned and displayed using the `.head()` method.","metadata":{}},{"cell_type":"code","source":"class DataProcessorEncode:\n    def __init__(self, cfg):\n        self.cfg = cfg\n\n    def encode_data(self, dataframe):\n        if self.cfg.one_hot_encoder:\n            object_cols = [col for col in dataframe.columns if dataframe[col].dtype in [\"object\",\"category\"]\n                           and dataframe[col].nunique()>2]\n            if len(object_cols) > 0:\n                dataframe = pd.get_dummies(dataframe, columns=object_cols)\n                print(\"One-hot encoding applied.\")\n            else:\n                print(\"Warning: No object or category columns found. One-hot encoding was not applied.\")\n\n        if self.cfg.label_encoder:\n            label_encoder = LabelEncoder()\n            object_cols = [col for col in dataframe.columns if dataframe[col].dtype in [\"object\",\"category\"]\n                           and dataframe[col].nunique()==2]\n            if len(object_cols) > 0:\n                for col in object_cols:\n                    dataframe[col] = label_encoder.fit_transform(dataframe[col])\n                print(\"Label encoding applied.\")\n            else:\n                print(\"Warning: No object or category columns found. Label encoding was not applied.\")\n                \n        if self.cfg.ordinal_encoder:\n            ordinal_encoder = {}\n            object_cols = dataframe.select_dtypes(include=['object', 'category']).columns\n            if len(object_cols) > 0:\n                for col in object_cols:\n                    unique_values = sorted(dataframe[col].unique())\n                    ordinal_encoder[col] = {value: index for index, value in enumerate(unique_values)}\n                    dataframe[col] = dataframe[col].map(ordinal_encoder[col])\n                print(\"Ordinal encoding applied.\")\n            else:\n                print(\"Warning: No object or category columns found. Ordinal encoding was not applied.\")\n\n\n        return dataframe\n\ndata_processor_encode = DataProcessorEncode(CFG())\ndf = data_processor_encode.encode_data(df)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:44:49.624015Z","iopub.execute_input":"2023-08-11T16:44:49.624300Z","iopub.status.idle":"2023-08-11T16:44:49.707626Z","shell.execute_reply.started":"2023-08-11T16:44:49.624274Z","shell.execute_reply":"2023-08-11T16:44:49.706610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Preprocessing: Feature Scaling**\n\nIn this section, we define a `DataProcessorScaled` class responsible for scaling numerical features within a given DataFrame. The class provides different scaling techniques based on the provided configuration (`cfg`).\n\n**1. Min-Max Scaling (`min_max_scaler`)**\n\nThe `min_max_scaler` method applies Min-Max scaling to numerical columns. It identifies numerical columns (`num_cols`) and scales their values to the range [0, 1] using the `MinMaxScaler` from scikit-learn. If applicable numerical columns are found, Min-Max scaling is applied; otherwise, a warning is issued.\n\n**2. Standard Scaling (`standard_scaler`)**\n\nThe `standard_scaler` method applies Standard Scaling (z-score normalization) to numerical columns. Similar to Min-Max scaling, it identifies numerical columns and scales their values to have zero mean and unit variance using the `StandardScaler` from scikit-learn.\n\n**3. Robust Scaling (`robust_scaler`)**\n\nThe `robust_scaler` method applies Robust Scaling to numerical columns. This technique is less sensitive to outliers than Min-Max or Standard Scaling. It scales the data using the median and interquartile range. Numerical columns are identified, and their values are scaled using the `RobustScaler` from scikit-learn.\n\nAfter defining these scaling methods, the `scaled_data` function takes a DataFrame as input and applies the specified scaling techniques based on the provided configuration (`cfg`).\n\nFinally, an instance of the `DataProcessorScaled` class is created with a configuration object (`CFG()`), and the `scaled_data` method is called on the input DataFrame `df`. The scaled DataFrame `df` is returned and displayed using the `.head()` method.","metadata":{}},{"cell_type":"code","source":"class DataProcessorScaled:\n    def __init__(self, cfg):\n        self.cfg = cfg\n\n    def scaled_data(self, dataframe):\n        if self.cfg.min_max_scaler:\n            num_cols = dataframe.select_dtypes(include=['int', 'float']).columns\n            if len(num_cols) > 0:\n                min_max_scaler = MinMaxScaler()\n                dataframe[num_cols] = min_max_scaler.fit_transform(dataframe[num_cols])\n                print(\"Min-Max scaling applied.\")\n            else:\n                print(\"Warning: No numerical columns found. Min-Max scaling was not applied.\")\n\n        if self.cfg.standard_scaler:\n            num_cols = dataframe.select_dtypes(include=['int', 'float']).columns\n            if len(num_cols) > 0:\n                standard_scaler = StandardScaler()\n                dataframe[num_cols] = standard_scaler.fit_transform(dataframe[num_cols])\n                print(\"Standard scaling applied.\")\n            else:\n                print(\"Warning: No numerical columns found. Standard scaling was not applied.\")\n\n        if self.cfg.robust_scaler:\n            num_cols = dataframe.select_dtypes(include=['int', 'float']).columns\n            if len(num_cols) > 0:\n                robust_scaler = RobustScaler()\n                dataframe[num_cols] = robust_scaler.fit_transform(dataframe[num_cols])\n                print(\"Robust scaling applied.\")\n            else:\n                print(\"Warning: No numerical columns found. Robust scaling was not applied.\")\n\n        return dataframe\n    \ndata_processor_scale = DataProcessorScaled(CFG())\ndf = data_processor_scale.scaled_data(df)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:44:49.709253Z","iopub.execute_input":"2023-08-11T16:44:49.709933Z","iopub.status.idle":"2023-08-11T16:44:49.775091Z","shell.execute_reply.started":"2023-08-11T16:44:49.709894Z","shell.execute_reply":"2023-08-11T16:44:49.774086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Splitting and Creation of Training/Test Sets**\n\nIn this section, we showcase the process of splitting your dataset into training and test subsets, a fundamental step for training and evaluating machine learning models. The provided code utilizes the `train_test_split` function to partition your dataset into two distinct subsets.\n\n- `test_size`: This parameter determines the proportion of the dataset that will be allocated to the test subset. In this case, a value of `0.2` has been used, signifying that 20% of the data is allocated to the test subset.\n- `random_state`: This parameter controls the randomness during the data splitting process. By setting this value, you can reproduce the same data split when you run the process again. Here, it is set to `42`.\n\nAdditionally, the target variable (`'Churn'`) is separated from the independent variables (`X`) and assigned to the dependent variable (`y`). These two subsets are then created to encompass training and testing data.","metadata":{}},{"cell_type":"code","source":"test_size = 0.2\nrandom_state = 42\nX = df.drop(columns=['Churn'])\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:44:49.776459Z","iopub.execute_input":"2023-08-11T16:44:49.777619Z","iopub.status.idle":"2023-08-11T16:44:49.789497Z","shell.execute_reply.started":"2023-08-11T16:44:49.777584Z","shell.execute_reply":"2023-08-11T16:44:49.788446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Handling Class Imbalance with SMOTE**\n\nIn this section, we address the issue of class imbalance in the dataset by employing the Synthetic Minority Over-sampling Technique (SMOTE). SMOTE is a technique used to balance the distribution of the classes by generating synthetic samples for the minority class.\n\nFirst, we import the necessary library, which is `SMOTE` from an undisclosed package (presumably `imblearn.over_sampling`). We initialize the `SMOTE` instance, setting the `random_state` to `42` to ensure reproducibility.\n\nNext, we apply SMOTE to the training data. The `fit_resample` method of the `SMOTE` instance is used for this purpose. It takes the original `X_train` (independent variables) and `y_train` (target variable) as inputs and returns resampled versions, `X_train_resampled` and `y_train_resampled`, respectively.","metadata":{}},{"cell_type":"code","source":"smote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:44:49.791506Z","iopub.execute_input":"2023-08-11T16:44:49.791911Z","iopub.status.idle":"2023-08-11T16:44:49.872822Z","shell.execute_reply.started":"2023-08-11T16:44:49.791873Z","shell.execute_reply":"2023-08-11T16:44:49.871886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model Training, Evaluation, and Result Visualization**\n\nIn this section, a function named `model` is defined to train, evaluate, and visualize the performance of a set of machine learning models. Additionally, another function named `model_to_dataframe` is provided to summarize the results of the model evaluations in a structured DataFrame.\n\n**1. `plot_confusion_matrix` Function**\n\nThis function, `plot_confusion_matrix`, takes true labels (`y_true`), predicted labels (`y_pred`), the model's name (`model_name`), and an axis (`ax`) to create and display a confusion matrix heatmap. The confusion matrix visually represents the distribution of correct and incorrect predictions for each class.\n\n**2. `model` Function**\n\nThe `model` function performs the following steps for each model specified in the `models` dictionary:\n\n- Fits the model using the resampled training data (`X_train_resampled` and `y_train_resampled`).\n- Generates predictions (`y_pred`) on the test set.\n- Calculates accuracy, precision, recall, and F1 score using various scoring metrics.\n- Stores the evaluation metrics in the `results` dictionary.\n- Calls the `plot_confusion_matrix` function to visualize the confusion matrix for the current model.\n\nFinally, it displays a grid of confusion matrix heatmaps for each model using `matplotlib` and returns the `results` dictionary containing the evaluation metrics.\n\n**3. `model_to_dataframe` Function**\n\nThis function, `model_to_dataframe`, utilizes the `model` function to evaluate the models' performance and converts the resulting `results` dictionary into a structured DataFrame. The DataFrame includes columns for 'Accuracy', 'Precision', 'Recall', and 'F1 Score' for each model.\n\nThe models are trained, evaluated, and ranked based on their accuracy in descending order. The resulting DataFrame provides an overview of the performance of different machine learning models on the test set.","metadata":{}},{"cell_type":"code","source":"def plot_confusion_matrix(y_true, y_pred, model_name, ax):\n    cm = confusion_matrix(y_true, y_pred)\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax)\n    ax.set_xlabel('Predicted')\n    ax.set_ylabel('True')\n    ax.set_title(f'Confusion Matrix - {model_name}')\n\ndef model(df, test_size=0.2, random_state=42):\n    \n    models = {\n        'LogisticRegression': LogisticRegression(max_iter=5000, solver='lbfgs'),\n        'RandomForest': RandomForestClassifier(),\n        'CatBoost': CatBoostClassifier(verbose=False),\n        'LightGBM': lgb.LGBMClassifier(),\n        'GradientBoosting': GradientBoostingClassifier()\n    }\n\n    results = {}\n    \n    fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(8, 15))\n    axes = axes.flatten()\n    results = {}\n\n    for i, (model_name, model) in enumerate(models.items()):\n        model.fit(X_train_resampled, y_train_resampled)\n        y_pred = model.predict(X_test)\n\n        accuracy = accuracy_score(y_test, y_pred)\n        precision = precision_score(y_test, y_pred, average='weighted')\n        recall = recall_score(y_test, y_pred, average='weighted')\n        f1 = f1_score(y_test, y_pred, average='weighted')\n\n        results[model_name] = {\n            'Accuracy': accuracy,\n            'Precision': precision,\n            'Recall': recall,\n            'F1 Score': f1\n        }\n\n        plot_confusion_matrix(y_test, y_pred, model_name, axes[i])\n\n    plt.tight_layout()\n    plt.show()\n    \n    return results\n\ndef model_to_dataframe(df, test_size=0.2, random_state=42):\n    results = model(df, test_size, random_state)\n    result_df = pd.DataFrame(results).T\n    result_df = result_df.sort_values(by='Accuracy', ascending=False)\n    return result_df\n\nmodel_to_dataframe(df)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:44:49.874264Z","iopub.execute_input":"2023-08-11T16:44:49.874605Z","iopub.status.idle":"2023-08-11T16:45:02.290859Z","shell.execute_reply.started":"2023-08-11T16:44:49.874574Z","shell.execute_reply":"2023-08-11T16:45:02.289476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Hyperparameter Tuning Function**\n\nThis function, named `hyperparameter_tuning`, is designed for performing hyperparameter tuning for a collection of machine learning models. It takes a dictionary of `models` along with corresponding `params` (hyperparameter grids), input features `X`, target variable `y`, and an optional `cv` (cross-validation) parameter.\n\nThe function iterates through each model specified in the `models` dictionary and performs hyperparameter tuning if hyperparameters are provided in the `params` dictionary. For each model, it follows these steps:\n\n1. If the model doesn't require hyperparameters, it skips tuning for that model and proceeds to the next one.\n\n2. If the model is an instance of `CatBoostClassifier`, it sets the verbosity level to 0 to suppress intermediate output during training.\n\n3. It uses `GridSearchCV` to perform cross-validated grid search over the specified hyperparameter grid (`params[model_name]`) for the given model. The `cv` parameter controls the number of folds in cross-validation, and `n_jobs=-1` indicates that computations are parallelized across all available CPU cores.\n\n4. The best parameters, best score, and best model obtained from the grid search are stored in the `best_models` dictionary.\n\n5. The function prints the best parameters and best score for the current model.\n\nFinally, the function returns a dictionary (`best_models`) containing the best parameters and scores for the models that were subject to hyperparameter tuning.\n\n**Note**: Ensure that you have the required libraries imported (`GridSearchCV`, `CatBoostClassifier`, etc.) and the necessary configurations set before calling this function.","metadata":{}},{"cell_type":"code","source":"def hyperparameter_tuning(models, params, X, y, cv=5):\n    best_models = {}\n    \n    for model_name, model in models.items():\n        if model_name not in params:\n            print(f\"Skipping Hyperparameter Tuning for {model_name} as it doesn't require hyperparameters.\")\n            continue\n        \n        print(f\"Performing Hyperparameter Tuning for {model_name}...\")\n        if isinstance(model, CatBoostClassifier):\n            model.set_params(verbose=0)\n        \n        grid_search_cv = GridSearchCV(model, params[model_name], cv=cv, n_jobs=-1)\n        grid_search_cv.fit(X, y)\n        \n        best_params = grid_search_cv.best_params_\n        best_score = grid_search_cv.best_score_\n        best_model = grid_search_cv.best_estimator_\n        \n        best_models[model_name] = {\n            'Best Parameters': best_params,\n            'Best Score': best_score,\n        }\n        \n        print(f\"{model_name} - Best Parameters:\", best_params)\n        print(f\"{model_name} - Best Score:\", best_score)\n        print(\"\\n\")\n    \n    return best_models","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:45:02.292752Z","iopub.execute_input":"2023-08-11T16:45:02.293527Z","iopub.status.idle":"2023-08-11T16:45:02.307591Z","shell.execute_reply.started":"2023-08-11T16:45:02.293482Z","shell.execute_reply":"2023-08-11T16:45:02.305746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model Hyperparameter Tuning and Result Analysis**\n\nIn this section, a collection of machine learning models are subjected to hyperparameter tuning, and the resulting best models are analyzed. The code provided showcases the process of defining models, specifying hyperparameter grids, performing hyperparameter tuning, and presenting the results.\n\n**1. Model and Hyperparameter Definitions**\n\nA dictionary named `models` contains a set of machine learning models, including 'LogisticRegression', 'RandomForest', 'CatBoost', 'LightGBM', and 'GradientBoosting'. Each model is instantiated with default hyperparameters.\n\nA corresponding dictionary named `params` holds hyperparameter grids for each model. Hyperparameters are defined within the grid to be tuned during the process. The ranges or options for hyperparameters are provided using `numpy` functions like `np.arange`.\n\n**2. Hyperparameter Tuning**\n\nThe `hyperparameter_tuning` function is called with the `models`, `params`, and resampled training data (`X_train_resampled` and `y_train_resampled`). This function iterates through each model, performing grid search cross-validation to find the best hyperparameters for each model.\n\n**3. Result Analysis**\n\nThe `results_to_dataframe` function converts the obtained best models and their information into a structured DataFrame. This DataFrame includes columns for 'Model', 'Best Parameters', and 'Best Score'.\n\nFinally, the `result_df` DataFrame is created using the `results_to_dataframe` function and sorted in descending order based on the 'Best Score'. This DataFrame provides an overview of the best hyperparameters and their corresponding scores for each model.\n\nThe entire process provides insight into the optimal hyperparameters for each model and allows for easy comparison of model performance.","metadata":{}},{"cell_type":"code","source":"models = {\n    'LogisticRegression': LogisticRegression(),\n    'RandomForest': RandomForestClassifier(),\n    'CatBoost': CatBoostClassifier(),\n    'LightGBM': lgb.LGBMClassifier(),\n    'GradientBoosting': GradientBoostingClassifier()\n}\n\nparams = {\n    'LogisticRegression': {'C': np.arange(1,11), 'penalty': ['l1', 'l2'], \n                           'max_iter': [5000], 'solver': ['lbfgs','saga']},\n    \n    'RandomForest': {'n_estimators': np.arange(100,1001,100), \n                     'criterion': ['gini','entropy'],\n                     'max_depth': np.arange(1,13,2)},\n    \n    'CatBoost': {'iterations': np.arange(100,501,100),\n                 'learning_rate': np.arange(0,0.101,0.05),\n                 'depth': np.arange(1,13,2)},\n    \n    'LightGBM': {'num_leaves': np.arange(100,1001,100), \n                 'max_depth': np.arange(1,13,2), 'verbose': [-1]},\n    \n    'GradientBoosting': {'n_estimators': np.arange(100,1001,100), \n                         'loss': ['log_loss','equivalent'],\n                         'learning_rate': np.arange(0,0.101,0.05)}\n}\n\nbest_models = hyperparameter_tuning(models, params, X_train_resampled, y_train_resampled)\n\ndef results_to_dataframe(best_models):\n    results = []\n\n    for model_name, model_info in best_models.items():\n        params = model_info['Best Parameters']\n        score = model_info['Best Score'],\n\n        results.append({\n            'Model': model_name,\n            'Best Parameters': params,\n            'Best Score': score\n        })\n\n    return pd.DataFrame(results)\n\nresult_df = results_to_dataframe(best_models)\nresult_df.sort_values(by='Best Score', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:45:02.309062Z","iopub.execute_input":"2023-08-11T16:45:02.310673Z","iopub.status.idle":"2023-08-11T18:01:20.191267Z","shell.execute_reply.started":"2023-08-11T16:45:02.310630Z","shell.execute_reply":"2023-08-11T18:01:20.190177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_feature_importance(model, feature_names, top_n=None):\n    if hasattr(model, 'feature_importances_'):\n        importances = model.feature_importances_\n    else:\n        raise ValueError(\"Feature importance not available for this model.\")\n    \n    if top_n is not None:\n        indices = np.argsort(importances)[::-1]\n        top_indices = indices[:top_n]\n        top_importances = importances[top_indices]\n        top_feature_names = np.array(feature_names)[top_indices]\n    else:\n        top_importances = importances\n        top_feature_names = np.array(feature_names)\n\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=top_importances, y=top_feature_names, palette='viridis')\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Feature')\n    plt.title('Top {} Feature Importance of the Model'.format(len(top_feature_names)))\n    plt.show()\n\nmodel = CatBoostClassifier(verbose=0)\nmodel.fit(X_train_resampled, y_train_resampled)\n\nplot_feature_importance(model, X.columns, top_n=len(X.columns))\n","metadata":{"execution":{"iopub.status.busy":"2023-08-11T18:06:49.800476Z","iopub.execute_input":"2023-08-11T18:06:49.800846Z","iopub.status.idle":"2023-08-11T18:06:59.312084Z","shell.execute_reply.started":"2023-08-11T18:06:49.800816Z","shell.execute_reply":"2023-08-11T18:06:59.311152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}